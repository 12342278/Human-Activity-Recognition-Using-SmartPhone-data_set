import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.ensemble import RandomForestClassifier as RFC

from sklearn.naive_bayes import GaussianNB
activity = pd.read_csv("train.csv.zip")
activity
tBodyAcc-mean()-X	tBodyAcc-mean()-Y	tBodyAcc-mean()-Z	tBodyAcc-std()-X	tBodyAcc-std()-Y	tBodyAcc-std()-Z	tBodyAcc-mad()-X	tBodyAcc-mad()-Y	tBodyAcc-mad()-Z	tBodyAcc-max()-X	...	fBodyBodyGyroJerkMag-kurtosis()	angle(tBodyAccMean,gravity)	angle(tBodyAccJerkMean),gravityMean)	angle(tBodyGyroMean,gravityMean)	angle(tBodyGyroJerkMean,gravityMean)	angle(X,gravityMean)	angle(Y,gravityMean)	angle(Z,gravityMean)	subject	Activity
0	0.288585	-0.020294	-0.132905	-0.995279	-0.983111	-0.913526	-0.995112	-0.983185	-0.923527	-0.934724	...	-0.710304	-0.112754	0.030400	-0.464761	-0.018446	-0.841247	0.179941	-0.058627	1	STANDING
1	0.278419	-0.016411	-0.123520	-0.998245	-0.975300	-0.960322	-0.998807	-0.974914	-0.957686	-0.943068	...	-0.861499	0.053477	-0.007435	-0.732626	0.703511	-0.844788	0.180289	-0.054317	1	STANDING
2	0.279653	-0.019467	-0.113462	-0.995380	-0.967187	-0.978944	-0.996520	-0.963668	-0.977469	-0.938692	...	-0.760104	-0.118559	0.177899	0.100699	0.808529	-0.848933	0.180637	-0.049118	1	STANDING
3	0.279174	-0.026201	-0.123283	-0.996091	-0.983403	-0.990675	-0.997099	-0.982750	-0.989302	-0.938692	...	-0.482845	-0.036788	-0.012892	0.640011	-0.485366	-0.848649	0.181935	-0.047663	1	STANDING
4	0.276629	-0.016570	-0.115362	-0.998139	-0.980817	-0.990482	-0.998321	-0.979672	-0.990441	-0.942469	...	-0.699205	0.123320	0.122542	0.693578	-0.615971	-0.847865	0.185151	-0.043892	1	STANDING
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
7347	0.299665	-0.057193	-0.181233	-0.195387	0.039905	0.077078	-0.282301	0.043616	0.060410	0.210795	...	-0.880324	-0.190437	0.829718	0.206972	-0.425619	-0.791883	0.238604	0.049819	30	WALKING_UPSTAIRS
7348	0.273853	-0.007749	-0.147468	-0.235309	0.004816	0.059280	-0.322552	-0.029456	0.080585	0.117440	...	-0.680744	0.064907	0.875679	-0.879033	0.400219	-0.771840	0.252676	0.050053	30	WALKING_UPSTAIRS
7349	0.273387	-0.017011	-0.045022	-0.218218	-0.103822	0.274533	-0.304515	-0.098913	0.332584	0.043999	...	-0.304029	0.052806	-0.266724	0.864404	0.701169	-0.779133	0.249145	0.040811	30	WALKING_UPSTAIRS
7350	0.289654	-0.018843	-0.158281	-0.219139	-0.111412	0.268893	-0.310487	-0.068200	0.319473	0.101702	...	-0.344314	-0.101360	0.700740	0.936674	-0.589479	-0.785181	0.246432	0.025339	30	WALKING_UPSTAIRS
7351	0.351503	-0.012423	-0.203867	-0.269270	-0.087212	0.177404	-0.377404	-0.038678	0.229430	0.269013	...	-0.740738	-0.280088	-0.007739	-0.056088	-0.616956	-0.783267	0.246809	0.036695	30	WALKING_UPSTAIRS
7352 rows × 563 columns

activity.isnull().sum().sum()
0
activity.describe()
tBodyAcc-mean()-X	tBodyAcc-mean()-Y	tBodyAcc-mean()-Z	tBodyAcc-std()-X	tBodyAcc-std()-Y	tBodyAcc-std()-Z	tBodyAcc-mad()-X	tBodyAcc-mad()-Y	tBodyAcc-mad()-Z	tBodyAcc-max()-X	...	fBodyBodyGyroJerkMag-skewness()	fBodyBodyGyroJerkMag-kurtosis()	angle(tBodyAccMean,gravity)	angle(tBodyAccJerkMean),gravityMean)	angle(tBodyGyroMean,gravityMean)	angle(tBodyGyroJerkMean,gravityMean)	angle(X,gravityMean)	angle(Y,gravityMean)	angle(Z,gravityMean)	subject
count	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	...	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000	7352.000000
mean	0.274488	-0.017695	-0.109141	-0.605438	-0.510938	-0.604754	-0.630512	-0.526907	-0.606150	-0.468604	...	-0.307009	-0.625294	0.008684	0.002186	0.008726	-0.005981	-0.489547	0.058593	-0.056515	17.413085
std	0.070261	0.040811	0.056635	0.448734	0.502645	0.418687	0.424073	0.485942	0.414122	0.544547	...	0.321011	0.307584	0.336787	0.448306	0.608303	0.477975	0.511807	0.297480	0.279122	8.975143
min	-1.000000	-1.000000	-1.000000	-1.000000	-0.999873	-1.000000	-1.000000	-1.000000	-1.000000	-1.000000	...	-0.995357	-0.999765	-0.976580	-1.000000	-1.000000	-1.000000	-1.000000	-1.000000	-1.000000	1.000000
25%	0.262975	-0.024863	-0.120993	-0.992754	-0.978129	-0.980233	-0.993591	-0.978162	-0.980251	-0.936219	...	-0.542602	-0.845573	-0.121527	-0.289549	-0.482273	-0.376341	-0.812065	-0.017885	-0.143414	8.000000
50%	0.277193	-0.017219	-0.108676	-0.946196	-0.851897	-0.859365	-0.950709	-0.857328	-0.857143	-0.881637	...	-0.343685	-0.711692	0.009509	0.008943	0.008735	-0.000368	-0.709417	0.182071	0.003181	19.000000
75%	0.288461	-0.010783	-0.097794	-0.242813	-0.034231	-0.262415	-0.292680	-0.066701	-0.265671	-0.017129	...	-0.126979	-0.503878	0.150865	0.292861	0.506187	0.359368	-0.509079	0.248353	0.107659	26.000000
max	1.000000	1.000000	1.000000	1.000000	0.916238	1.000000	1.000000	0.967664	1.000000	1.000000	...	0.989538	0.956845	1.000000	1.000000	0.998702	0.996078	1.000000	0.478157	1.000000	30.000000
8 rows × 562 columns

plt.figure(figsize=(15, 6))
sns.countplot(x='Activity', data=activity);

X = activity.drop(columns=['subject', 'Activity'])
y = activity['Activity']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
((4411, 561), (2941, 561), (4411,), (2941,))
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline

from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier as KNN
steps1 = [('dimension reduction', PCA(n_components=0.9)),
          ('classifier', DTC(criterion='gini'))] 

steps2 = [('dimension reduction', PCA(n_components=0.9)),
          ('classifier', RFC(n_estimators=10,
                             criterion='gini'))] 

steps3 = [('dimension reduction', PCA(n_components=0.9)),
          ('classifier', SVC(kernel='rbf'))] 

steps4 = [('dimension reduction', PCA(n_components=0.9)),
          ('classifier', KNN(n_neighbors=5))] 

steps5 = [('dimension reduction', PCA(n_components=0.9)),
          ('classifier', KNN(n_neighbors=7))] 


clf1 = Pipeline(steps=steps1).fit(X_train, y_train)
clf2 = Pipeline(steps=steps2).fit(X_train, y_train)
clf3 = Pipeline(steps=steps3).fit(X_train, y_train)
clf4 = Pipeline(steps=steps4).fit(X_train, y_train)
clf5 = Pipeline(steps=steps5).fit(X_train, y_train)
y_pred1 = clf1.predict(X_test)
y_pred2 = clf2.predict(X_test)
y_pred3 = clf3.predict(X_test)
y_pred4 = clf4.predict(X_test)
y_pred5 = clf5.predict(X_test)
from sklearn.metrics import classification_report
print(classification_report(y_true=y_test, y_pred=y_pred1))
                    precision    recall  f1-score   support

            LAYING       0.98      0.98      0.98       546
           SITTING       0.75      0.75      0.75       515
          STANDING       0.78      0.78      0.78       555
           WALKING       0.90      0.93      0.92       496
WALKING_DOWNSTAIRS       0.87      0.86      0.87       393
  WALKING_UPSTAIRS       0.87      0.85      0.86       436

          accuracy                           0.86      2941
         macro avg       0.86      0.86      0.86      2941
      weighted avg       0.86      0.86      0.86      2941

print(classification_report(y_true=y_test, y_pred=y_pred2))
                    precision    recall  f1-score   support

            LAYING       0.98      1.00      0.99       546
           SITTING       0.83      0.80      0.82       515
          STANDING       0.83      0.85      0.84       555
           WALKING       0.93      0.97      0.95       496
WALKING_DOWNSTAIRS       0.94      0.91      0.92       393
  WALKING_UPSTAIRS       0.93      0.92      0.93       436

          accuracy                           0.91      2941
         macro avg       0.91      0.91      0.91      2941
      weighted avg       0.91      0.91      0.91      2941

print(classification_report(y_true=y_test, y_pred=y_pred3))
                    precision    recall  f1-score   support

            LAYING       0.99      1.00      1.00       546
           SITTING       0.89      0.82      0.85       515
          STANDING       0.85      0.91      0.88       555
           WALKING       0.98      1.00      0.99       496
WALKING_DOWNSTAIRS       0.99      0.97      0.98       393
  WALKING_UPSTAIRS       0.98      0.99      0.98       436

          accuracy                           0.94      2941
         macro avg       0.95      0.95      0.95      2941
      weighted avg       0.94      0.94      0.94      2941

print(classification_report(y_true=y_test, y_pred=y_pred4))
                    precision    recall  f1-score   support

            LAYING       0.99      1.00      1.00       546
           SITTING       0.89      0.81      0.85       515
          STANDING       0.84      0.91      0.88       555
           WALKING       0.99      0.99      0.99       496
WALKING_DOWNSTAIRS       0.99      0.97      0.98       393
  WALKING_UPSTAIRS       0.97      0.99      0.98       436

          accuracy                           0.94      2941
         macro avg       0.95      0.94      0.95      2941
      weighted avg       0.94      0.94      0.94      2941

print(classification_report(y_true=y_test, y_pred=y_pred5))
                    precision    recall  f1-score   support

            LAYING       0.99      1.00      1.00       546
           SITTING       0.88      0.79      0.84       515
          STANDING       0.83      0.90      0.87       555
           WALKING       0.98      0.99      0.99       496
WALKING_DOWNSTAIRS       0.99      0.96      0.98       393
  WALKING_UPSTAIRS       0.97      0.99      0.98       436

          accuracy                           0.94      2941
         macro avg       0.94      0.94      0.94      2941
      weighted avg       0.94      0.94      0.94      2941

acc = []
n_es = []

for n in range(2, 31):
    steps = [('dimension reduction', PCA(n_components=0.95)),
             ('classifier', RFC(n_estimators=n,
                                criterion='gini'))]
    clf = Pipeline(steps=steps).fit(X_train, y_train)
    n_es.append(n)
    acc.append(accuracy_score(y_true=y_test,
                              y_pred=clf.predict(X_test)))
    
    print(n, acc[-1])
2 0.7337640258415505
3 0.7983679020741244
4 0.8520911254675281
5 0.8599115946956818
6 0.8786127167630058
7 0.8905134308058483
8 0.9160149608976539
9 0.9047942876572594
10 0.9143148588915335
11 0.9149948996939816
12 0.9126147568854132
13 0.9132947976878613
14 0.912274736484189
15 0.9183951037062223
16 0.9197551853111187
17 0.9197551853111187
18 0.9272356341380483
19 0.9241754505270316
20 0.9268956137368242
21 0.9275756545392724
22 0.9224753485209113
23 0.9289357361441687
24 0.9353961237674261
25 0.9381162869772186
26 0.9289357361441687
27 0.9292757565453927
28 0.9316558993539612
29 0.9309758585515131
30 0.9245154709282557
plt.plot(n_es, acc);

from sklearn.model_selection import KFold
kf = KFold(n_splits=10)
kf.get_n_splits(X, y)
10
acc = []
for train_index, test_index in kf.split(np.asarray(X)):
    X_train, X_test = np.asarray(X)[train_index, :], np.asarray(X)[test_index, :]
    y_train, y_test = np.asarray(y)[train_index], np.asarray(y)[test_index]
    
    steps = [('dimension reduction', PCA(n_components=0.9)),
             ('classifier', KNN(n_neighbors=5))]
    
    clf = Pipeline(steps=steps).fit(X_train, y_train)
    acc.append(accuracy_score(y_true=y_test,
                              y_pred=clf.predict(X_test)))
100*np.mean(acc), 100*np.std(acc)
(87.37769890564921, 3.495015116341529)
